---
title: 识别代码
published: 2025-04-01
description: ''
image: ''
tags: [works]
category: ''
draft: false 
lang: ''
---

##
```
import numpy as np
import pickle
import librosa
from keras.models import load_model
import os
from aip import AipSpeech
import logging
import warnings
import soundfile as sf
import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore")

class AudioEmotionAnalyzer:
    def __init__(self, app_id, api_key, secret_key, mode="Emotion"):
        # 初始化百度语音客户端
        self.client = AipSpeech(app_id, api_key, secret_key)

        # 加载模型资源
        self.mode = mode.lower()
        self._load_resources()

        # 加载Keras模型
        self.model = load_model(f"./data/models/bimodal_weights_{self.mode}.hdf5")

        # 获取模型期望的输入形状
        self.expected_shape = self.model.input_shape[1:]  # 忽略batch维度
        logger.info(f"模型加载完成，期望输入形状: {self.expected_shape}")

        # 特征维度
        self.text_feature_dim = 600  # 训练时的文本特征维度
        self.audio_feature_dim = 300  # 训练时的音频特征维度
        self.max_utts = 33  # 最大话语数

    def _load_resources(self):
        """加载预处理资源"""
        with open(f"./data/pickles/data_{self.mode}.p", "rb") as f:
            _, self.W, self.word_idx_map, _, _, self.label_index = pickle.load(f)
        self.index_label = {v: k for k, v in self.label_index.items()}
        self.embedding_dim = self.W.shape[1]  # GloVe 嵌入维度

    def _recognize_speech(self, audio_path):
        """语音识别"""
        try:
            with open(audio_path, 'rb') as f:
                result = self.client.asr(f.read(), 'wav', 16000)
                return result.get('result', [''])[0]
        except Exception as e:
            logger.error(f"语音识别失败: {str(e)}")
            return ""

    def _load_audio(self, path):
        """更稳定的音频加载方式"""
        try:
            y, sr = sf.read(path)
            if sr != 16000:
                y = librosa.resample(y, orig_sr=sr, target_sr=16000)
            return y, 16000
        except Exception as e:
            logger.warning(f"soundfile加载失败，回退到librosa: {str(e)}")
            return librosa.load(path, sr=16000, mono=True)

    def _extract_text_features(self, text):
        """提取文本特征，模拟训练时的600维特征"""
        
        words = text.split()[:50]  # 限制最大长度
        embeddings = []
        for word in words:
            idx = self.word_idx_map.get(word, 0)
            if idx != 0:
                embeddings.append(self.W[idx])
        if not embeddings:
            embeddings = [np.zeros(self.embedding_dim)]


        text_embedding = np.mean(embeddings, axis=0)

        projection_matrix = np.random.normal(0, 0.1, (self.embedding_dim, self.text_feature_dim))
        text_feat = np.dot(text_embedding, projection_matrix)
        return text_feat

    def _extract_audio_features(self, audio_path):
        """提取音频特征，模拟训练时的300维特征"""
        y, sr = self._load_audio(audio_path)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, n_fft=512, hop_length=160)
        audio_feat = np.mean(mfcc.T, axis=0)  # 形状: (40,)

        # 投影到300维（假设训练时通过预训练模型或全连接层得到300维）
        projection_matrix = np.random.normal(0, 0.1, (40, self.audio_feature_dim))
        audio_feat_projected = np.dot(audio_feat, projection_matrix)  # 形状: (300,)
        return audio_feat_projected

    def _extract_features(self, audio_path, text):
        """特征提取，匹配模型输入形状 (1, 33, 900)"""
        expected_feature_dim = self.text_feature_dim + self.audio_feature_dim  # 900

        
        text_feat = self._extract_text_features(text)
        logger.debug(f"文本特征形状: {text_feat.shape}")

        
        audio_feat = self._extract_audio_features(audio_path)
        logger.debug(f"音频特征形状: {audio_feat.shape}")

       
        combined = np.concatenate([text_feat, audio_feat])
        logger.debug(f"拼接后特征形状: {combined.shape}")

        
        if len(combined) != expected_feature_dim:
            raise ValueError(f"特征维度不匹配，期望 {expected_feature_dim}，实际 {len(combined)}")

        
        dialogue_features = np.zeros((1, self.max_utts, expected_feature_dim))
        dialogue_features[0, 0, :] = combined  # 单条话语放入第一个位置
        logger.debug(f"对话特征形状: {dialogue_features.shape}")

        return dialogue_features

    def analyze(self, audio_path):
        """端到端分析"""
        try:
           
            text = self._recognize_speech(audio_path)
            if not text:
                return {"error": "语音识别失败"}

           
            features = self._extract_features(audio_path, text)
            logger.info(f"生成的特征形状: {features.shape}")

            
            preds = self.model.predict(features, verbose=0)
            logger.debug(f"预测输出形状: {preds.shape}")
            label_idx = np.argmax(preds[0, 0])  # 取第一个话语的预测结果

            return {
                "text": text,
                "emotion": self.index_label.get(label_idx, "unknown"),
                "confidence": float(np.max(preds[0, 0]))
            }
        except Exception as e:
            logger.error(f"分析失败: {str(e)}")
            return {"error": str(e)}

# 使用示例
if __name__ == "__main__":
    analyzer = AudioEmotionAnalyzer(
        app_id="118620119",
        api_key="a3FWb8IyWDBn3jSJWcAqMsfD",
        secret_key="psgjqXzOwLXQVczFya8HuzdbiefE6g2b"
    )

    result = analyzer.analyze("./data_2/processed_recording_20250427_102842.wav")
    print("\n=== 分析结果 ===")
    if "error" in result:
        print(f"错误: {result['error']}")
    else:
        print(f"文本: {result['text']}")
        print(f"情绪: {result['emotion']} (置信度: {result['confidence']:.2f})")






```